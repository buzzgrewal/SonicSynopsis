{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yt_dlp transformers datasets torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:41:11.429683Z",
          "iopub.execute_input": "2025-02-05T18:41:11.430034Z",
          "iopub.status.idle": "2025-02-05T18:41:17.943687Z",
          "shell.execute_reply.started": "2025-02-05T18:41:11.430004Z",
          "shell.execute_reply": "2025-02-05T18:41:17.942688Z"
        },
        "id": "nDnI2BFEjNvR",
        "outputId": "af35e1b5-12e5-4e43-902b-dc7df04e6e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting yt_dlp\n  Downloading yt_dlp-2025.1.26-py3-none-any.whl.metadata (172 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading yt_dlp-2025.1.26-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: yt_dlp\nSuccessfully installed yt_dlp-2025.1.26\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import yt_dlp\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:41:21.010660Z",
          "iopub.execute_input": "2025-02-05T18:41:21.010989Z",
          "iopub.status.idle": "2025-02-05T18:41:41.958046Z",
          "shell.execute_reply.started": "2025-02-05T18:41:21.010957Z",
          "shell.execute_reply": "2025-02-05T18:41:41.957303Z"
        },
        "id": "EMSivRM9jNvX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def download_youtube_audio(youtube_url):\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': os.path.join(temp_dir, 'audio.%(ext)s'),\n",
        "        'quiet': True,\n",
        "        'no_warnings': True,\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(youtube_url, download=True)\n",
        "            audio_file = os.path.join(temp_dir, \"audio.mp3\")  # Ensure correct extension\n",
        "            return audio_file\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading audio: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:41:45.224489Z",
          "iopub.execute_input": "2025-02-05T18:41:45.225133Z",
          "iopub.status.idle": "2025-02-05T18:41:45.230301Z",
          "shell.execute_reply.started": "2025-02-05T18:41:45.225106Z",
          "shell.execute_reply": "2025-02-05T18:41:45.229310Z"
        },
        "id": "n9rRbk-GjNvZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "audio = download_youtube_audio(\"https://youtu.be/Fbg7ChsjmEA?si=ANh-EYzZty81-Jhx\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:41:49.218623Z",
          "iopub.execute_input": "2025-02-05T18:41:49.219145Z",
          "iopub.status.idle": "2025-02-05T18:42:15.061581Z",
          "shell.execute_reply.started": "2025-02-05T18:41:49.219099Z",
          "shell.execute_reply": "2025-02-05T18:42:15.060627Z"
        },
        "id": "X4eYSL2VjNvh",
        "outputId": "e6239be4-2031-4314-8ce4-c9238ddd602a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                                                           \r",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if audio:\n",
        "    target_path = os.path.join(os.getcwd(), \"audio.mp3\")\n",
        "    shutil.move(audio, target_path)\n",
        "    print(f\"Audio saved to: {target_path}\")\n",
        "else:\n",
        "    print(\"Failed to download audio.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:42:18.312976Z",
          "iopub.execute_input": "2025-02-05T18:42:18.313285Z",
          "iopub.status.idle": "2025-02-05T18:42:18.340005Z",
          "shell.execute_reply.started": "2025-02-05T18:42:18.313262Z",
          "shell.execute_reply": "2025-02-05T18:42:18.339170Z"
        },
        "id": "u1C72AgejNvk",
        "outputId": "4282285f-8e1a-4f5a-d045-8e14632e897b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Audio saved to: /kaggle/working/audio.mp3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:42:22.190175Z",
          "iopub.execute_input": "2025-02-05T18:42:22.190471Z",
          "iopub.status.idle": "2025-02-05T18:42:22.248268Z",
          "shell.execute_reply.started": "2025-02-05T18:42:22.190448Z",
          "shell.execute_reply": "2025-02-05T18:42:22.247455Z"
        },
        "id": "_9AtfAzqjNvl",
        "outputId": "0f692007-dc50-434b-962c-257593ef5b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "print(torch_dtype)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:43:07.259971Z",
          "iopub.execute_input": "2025-02-05T18:43:07.260299Z",
          "iopub.status.idle": "2025-02-05T18:43:07.265053Z",
          "shell.execute_reply.started": "2025-02-05T18:43:07.260277Z",
          "shell.execute_reply": "2025-02-05T18:43:07.264203Z"
        },
        "id": "b3u5xLfdjNvm",
        "outputId": "82c477c9-0d3e-471b-cc74-52a034c2b37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.float16\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:43:24.039867Z",
          "iopub.execute_input": "2025-02-05T18:43:24.040214Z",
          "iopub.status.idle": "2025-02-05T18:44:07.094304Z",
          "shell.execute_reply.started": "2025-02-05T18:43:24.040189Z",
          "shell.execute_reply": "2025-02-05T18:44:07.093454Z"
        },
        "colab": {
          "referenced_widgets": [
            "523a15ab2efb43629803a1092d16e4cf",
            "d8d71662e7a04f3787637762311512ad",
            "a84a952974f3428496151f4e16b0e3bb"
          ]
        },
        "id": "DVlvypr1jNvn",
        "outputId": "ec249938-eadc-4ec8-c357-21af06af168a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "523a15ab2efb43629803a1092d16e4cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8d71662e7a04f3787637762311512ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/3.77k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a84a952974f3428496151f4e16b0e3bb"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 1280)\n      (layers): ModuleList(\n        (0-31): 32 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n      (layers): ModuleList(\n        (0-3): 4 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "processor = AutoProcessor.from_pretrained(model_id)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:44:11.560759Z",
          "iopub.execute_input": "2025-02-05T18:44:11.561095Z",
          "iopub.status.idle": "2025-02-05T18:44:15.174467Z",
          "shell.execute_reply.started": "2025-02-05T18:44:11.561067Z",
          "shell.execute_reply": "2025-02-05T18:44:15.173791Z"
        },
        "colab": {
          "referenced_widgets": [
            "6a24cf05ec8c40bcaddb644596b63404",
            "6ea3d6a68e4f4c8682e1ea14881be6f6",
            "73fbf4326716450b8424255ebc666440",
            "b97df5979f4b4261b2322716032647c5",
            "a7f48226826f4c4ca57e63bf468d82c4",
            "689e17a3ddef4346bd18c3a11a537743",
            "626feb57c9384c08b1dce7c5fe15ee2f",
            "b86840e113a441de81564b3c6b42dff1"
          ]
        },
        "id": "jjAVRekjjNvq",
        "outputId": "52dc54eb-35cc-41f9-a1e4-6f922b007659"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a24cf05ec8c40bcaddb644596b63404"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ea3d6a68e4f4c8682e1ea14881be6f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73fbf4326716450b8424255ebc666440"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.71M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b97df5979f4b4261b2322716032647c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7f48226826f4c4ca57e63bf468d82c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "689e17a3ddef4346bd18c3a11a537743"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "626feb57c9384c08b1dce7c5fe15ee2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b86840e113a441de81564b3c6b42dff1"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:44:27.836582Z",
          "iopub.execute_input": "2025-02-05T18:44:27.836871Z",
          "iopub.status.idle": "2025-02-05T18:44:27.842746Z",
          "shell.execute_reply.started": "2025-02-05T18:44:27.836849Z",
          "shell.execute_reply": "2025-02-05T18:44:27.841968Z"
        },
        "id": "rODd378hjNvs",
        "outputId": "3462a7b9-3f9c-4e34-9fd2-45f51d489746"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:44:31.264193Z",
          "iopub.execute_input": "2025-02-05T18:44:31.264485Z",
          "iopub.status.idle": "2025-02-05T18:44:43.160834Z",
          "shell.execute_reply.started": "2025-02-05T18:44:31.264463Z",
          "shell.execute_reply": "2025-02-05T18:44:43.160196Z"
        },
        "colab": {
          "referenced_widgets": [
            "593c05814ee04434a3b1c6e9a6ea187a",
            "69231579890d412ba47e3f7b80454bcb",
            "e55b62d13a7b46e6ba673a080e54cb95"
          ]
        },
        "id": "ZtCr_k5ujNvu",
        "outputId": "8ab1b5b1-8e26-4aaf-ebea-2f0d2495db95"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/480 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "593c05814ee04434a3b1c6e9a6ea187a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "(…)-00000-of-00001-913508124a40cb97.parquet:   0%|          | 0.00/1.98M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69231579890d412ba47e3f7b80454bcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e55b62d13a7b46e6ba673a080e54cb95"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(target_path,return_timestamps=True)\n",
        "print(result[\"text\"])\n",
        "transcript = result[\"text\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:53:54.325384Z",
          "iopub.execute_input": "2025-02-05T18:53:54.325686Z",
          "iopub.status.idle": "2025-02-05T18:54:34.526985Z",
          "shell.execute_reply.started": "2025-02-05T18:53:54.325663Z",
          "shell.execute_reply": "2025-02-05T18:54:34.526312Z"
        },
        "id": "_QIO5aGijNvv",
        "outputId": "9e32b3e1-2b56-44f3-ec9b-b3e57fa281ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\nWhisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " Here it is, NVIDIA's new RTX 5080. Compared to its predecessor, the 4080 Super, it is around 15% faster in games that use its new features. It's faster in AI, as long as you don't need more than 16 gigs of RAM for your model. It's more efficient, even though it draws more power overall. And it's got a great cooler design, as long as you don't mind heating up your CPU. And that's it. That's the most positive spin we can put on this thing. Why the do we need all of these caveats when we say the new one is better than the old one? I mean, isn't the point of the new one to be clearly and substantially better? Well, it is better. In cranked out Cyberpunk at 4K, it not only beats the 4080 Super, but it even gives the 4090 a run for its money. But it's such a mixed bag that in other situations, it barely outcompetes the 7900 XTX, the flagship GPU that made AMD think, You know, maybe we just shouldn't do flagships anymore. Hey, glass half full? At least they didn't raise the price. And we did find some situations where the 5080 offers a pretty compelling use case. And if you don't mind the weirdness of generated frames, maybe Nvidia's onto something with the neural rendering stuff. We're gonna tell you more about it after I nearly render a better way to sell your used hardware. Is it available? Yeah. Is it available? Yeah. Again? Is it available? Just let our sponsor Jawa by your old GPUs. No need for more stupid questions. Yeah, so are you interested? Before we explore the wildlands of AI assisted rendering, let's get our bearings with some good old fashioned 1440p results. All raster, no DLS upscaling or ray tracing. In Cyberpunk, the 5080 makes a great first impression, with a 20% lead over the 4080 Super. Then in Alan Wake, the 5080 brings a decent generational uplift that brings 1% low FPS into the triple digits. But also it only barely outpaces the 7900 XTX here. In F124, things look even less exciting, with the 5080 failing to overtake AMD's now 2-year-old flagship, and only offering a measly 7-ish percent uplift over the 4080 Super. Does it get any better? No. No again, and triple no. Why does the last gen 4070 Ti Super make everything else here look like an overpriced joke? At least Black Myth Wukong provides a reason for the 5080 to exist, but overall I'm just kinda getting sad reading you all these charts, and you can expect a roughly 10% uplift over the 4080 Super and just 7% over the 7900 XTX. If you're curious about the 4080 non-super, there's a reason we called it the 4080 cheaper. It performs basically identically, and you can't buy it anymore, so we opted to use our limited graph space for older cards that might actually be worth upgrading from all right well maybe the story changes at 4k oh yeah okay so it's a bit happier but this feels a little bit like going from bridge to terabithia to dead poet society on average the 5080 takes a commanding 18 lead over its predecessor which is good but that parade gets somewhat rained on by its mere 10 lead over the 7900 xtx there are a couple of standouts that merit specific mention here black myth wukong has the 5080 taking a price appropriate lead over the 7900 xtx with a worthwhile uplift over the 4080 super and that's also true in cyberpunk where again the 5080 nearly catches the 4090 losing only in the less important average fps but that one result there it's less of a trend and more of just an outlier in other titles it seems like the 5080 is content to just be its disappointing self. Better, but not better enough. Of course, Nvidia doesn't think that traditional raster rendering is the way forward. No, their way forward is with the AI GPU hardware that made them the most valuable company in the world, and any conflicting data would almost certainly mean a cratering stock price. So, with the AI boom in mind, Nvidia focused most of their architectural upgrades on that. If you want to learn more about the architectural and technology upgrades we highly recommend you check out our 5090 review where we spend a bit more time explaining what everything means what really matters for gamers is how this car differs from the 4080 and 4080 super it's not much we're still on tsmc's 4n process node but at the same msrp we get four more streaming multi-processors in the 4080 super along with a clock speed bump and a memory swap from 16 gigabytes of gdr6x to 16 gigabytes of gdr7 that all explains why raw gaming performance isn't improved much. The bigger improvements come in the form of next generation Nvidia ray tracing, RT cores, and AI accelerating tensor cores. The tensor cores also now have hardware support for FP4 calculations for improved performance and memory usage with lightweight AI models and are more tightly integrated with CUDA cores to make way for the sci-fi sounding neural rendering. For hypothetical future gamers, these neural rendering techniques actually sound really cool and they might turn out great. But gamers shouldn't feel pressure to upgrade until a must-play title that uses them hits the Steam Store. Instead, feel pressured to buy our Precision Multi-Bit Screwdriver Bundle from LTTstore.com. Anywho, those working with high-end video might upgrade for the new media engine on the 5000 series that provides support for 422 chroma subsampling. In a nutshell, this means much higher quality hardware accelerated video encoding, which will be a boon for professionals and anyone who likes streaming their games to a handheld or lightweight decoding box. We're also stoked about DisplayPort 2.1 UHBR20. The 5080 can drive 4K 240Hz with no display stream compression and even go higher with DSC, which doesn't impact image quality as far as our eyes can perceive. That means no compromises between image quality and latency and responsiveness. Let's move on to ray tracing, where, yeah, NVIDIA is really just competing against themselves here. But as more big titles list ray tracing as a minimum requirement, it's becoming something that just can't be brushed off, which is a problem for AMD. At 1440p, while AMD can survive lightly ray-traced games, the path tracing found in Alan Wake and BlackMyth Wukong make AMD look like an 8-year-old kid made it onto the field at an NFL game. Hopefully, that will change with AMD's 9000 series of GPUs, but we don't know for sure. Today, across our suite of games, the 5080 maintains the same performance margin over the 4080 Super that we saw in our raster tests, but fails to scale as well as the 4090 does into 4K. Maybe the 4090's extra RT cores give it the edge, or maybe it's the VRAM. Check out this result in Alan Wake 2. Why did so many cards fail miserably? Guess now. Did you say lack of VRAM? Well, you're right. And if you didn't say lack of VRAM, pretend that you did. 4K path traced Alan Wake asks for over 15 gigabytes of VRAM on 50 series cards and over 16 on AMD cards. Anything less and it's wholly unplayable. While we can't say that it causes a performance hit on the 5080, it highlights just how skimpy 16GB of VRAM feels on a $1,000 card, and how close we are to a future where it's necessary for high-end gaming. And the more frustrating thing is that if you want more VRAM from Team Green, you literally have to spend twice as much for a 5090 or go back a generation or two. I digress. The rest of the 4K RT results show that the 5080 is still a mean green ray tracing machine, it's just not that much meaner than the 4080 Super. Another disappointing result for the 5000 series. For productivity, the story is more mixed. While it doesn't do much to impress in our Blender render, in Premiere Pro we can see that the updated media engine on the new 50 series puts in the work, beating the 4090 by 5%, with a similar lead in DaVinci Resolve. We'd love to talk about improvements to encoding, but we're working on our benchmarks for that. Let us cook. But who needs to shoot and edit videos when you could just generate garbage while wasting precious earthly resources. In AI, the 5080 manages a 20% lead over the 4080 Super in our text generation benchmarks, with the only hiccup being a smaller lead in our Fi 3.5. In our stable diffusion image generation benchmarks, we see 16-ish percent improvement over the 4080 Super, which is cool, I guess. Sadly, a lack of VRAM will limit the models you can play with. Womp womp. But this is positioned as a gaming card, so let's talk about the AI that makes games run gooder. DLSS4 is a massive overhaul from DLSS3. It switches from a CNN model to a transformer model. If you want to learn more about the theory, you should check out our 5090 review. What it means in practice is that DLSS4 allows for better image quality at the same settings, with some caveats. Of course, it's the 50 series. There's going to be caveats. There's a performance hit using the new transformer model. While the effect is mild for things like super resolution, Digital Foundry saw that older cards can take a real hit from enabling certain features with the new model. But it scales better, which means that the 50 series and all of its Tensor Core chutzpah generates up to three frames for every rendered frame, essentially quadrupling your FPS. Multi-frame gen is impressive, and despite generating so many frames, the tech does not impact latency substantially, at least over top of your base frame rate. Sadly, more frames is not more performance. If you were to run a game natively at 30 FPS, each frame would have 33 milliseconds of latency. Running at 60 FPS would half your latency to 16.66, doubling your FPS again, 8.33 milliseconds. You might see where I'm going with this. If you quadruple your 30 FPS game with FrameGen, you will still have that 33.33 milliseconds of latency, which is 25 milliseconds worse than rendering 120 FPS natively, a large disparity. Meanwhile, if you quadruple your 120 FPS game to 480 FPS, well, that only results in about a 6 millisecond difference in latency, which is far less noticeable. But here's the kicker. MFG has a performance overhead that will reduce your base frame rate. And the more frames you gen, the more overhead you need to contend with. That's why we don't see a literal quadrupling of frames. This means that using it to bump up an already low frame rate will make it feel even worse to play as it adds more latency. There's a ton of nuance to this tech, and we would recommend checking out Hardware Unboxed's excellent video on it. The good news is that multi-frame gen scales about as well on the 5080 as it did on the 5090. Hopefully this trend carries forward to the lower 50 series because it did not carry over to the lower end of the 40 series. Sheesh. So TLDR, the higher your base frame rate, the better your frame gen will feel, but the less you need it. NVIDIA likes to pretend that frame gen is comparable to native rendering in their marketing, but it's just not. Should you forego MFG and just stick with the new DLSS4 transformer model, on the 5080, you'll see about a 5% lower frame rate for the improved image quality. And we think it's worth the trade-off. Now, all of this talk of frame rates and latency might have you wondering, how does the 5080 fare in eSports? Well, it does a dang good job. We didn't test the rest of the lineup today because, frankly, any of the high-end cards will give you a great experience in eSports titles. But this is a reminder that despite our dour and disappointed tone, it's still a very high-end, very performant card. And that also means it has big power draw. The 5080 gets the same sleek and innovative double flow-through cooler as the 5090, allowing it to stay at a reasonable 70 degrees in combustor, despite pulling as much as 403 watts. If you're going to small form factor, you're far less likely to bump up against the thermal limits of this car than with the 5090. When gaming, the average temps drop further to 65 degrees thanks to the lower power budget. In F124, we see the 5080 pulling an average of 337 watts, nearly 15% more than the 4080 Super. This nets around 14% more frames, which indicates no efficiency gains, just like we saw in the 5090. Bummer. Feeling deflated? Me too. Taking everything into account, we have a next generation card that outperforms its last gen counterpart by 10 to 20% using roughly 15% more power. I mean, at least the price is the same, that's good. Even then, it feels like it could have been a mid-cycle refresh or a 4080 Ti, given it doesn't catch the 4090 in raw performance, something we really hoped it would do. At best, if you're considering the 4080 Super, The 5080 might be enough to push you over the edge to buy, at least at MSRP, but that's the hardest working S in the world. Because last gen cards are nearly impossible to find at Nvidia's suggested price. And if the 5090 is anything to go by, this generation will be the same or even worse. It feels like Nvidia just doesn't care much about gamers. That's because they don't. They care about AI. That's what made them one of the most valuable companies in the world. And they're trying to use their large market share to move the market in their direction. They're saying, no, no, no, no, no, you don't want more raster. What the industry wants, nay, what it needs is AI. And AI just so happens to need the proprietary features enabled by the chips that we produce. I get it. If they were to admit that AI hardware wasn't the solution to every conceivable problem, then it would crater their stock price, a thing that actually just happened on Monday. The moment a Chinese AI company with limited access to NVIDIA's best AI hardware supposedly overtook Chad GPT, Team Green's stock dropped 17 points. If the hype for AI, and more specifically to them, AI hardware deflates, Jensen might have to trade his crocodile leather jacket for a regular leather jacket. But with all of that said, without a clear competitor in the GPU space, NVIDIA will still sell a buttload of these to gamers and will continue to drive the industry in the direction of their choosing. So until AMD or Intel brings the heat, this is what we got. Really kind of feels like the Intel, hey, that's a nice quad core you got there. How about this shiny new quad core days? All I can do is hope at this point then that NVIDIA can nearly render some more innovative ideas before they suffer the same eventual fate that Intel did. Just like I'm going to nearly render this Segway to our sponsor. What? You expected an AI to come up with a new Segway? I mean, it was trained on my Segway to our sponsor. Bessie, hiding your stash in your shoes is nothing special. Keeping you dry while stepping through water? that's what Vessy can offer. Still dry. They're not your typical clunky rubber rim boot. They kind that leave your feet looking and smelling like blue chips. Not only to keep your socks dry, but also stay breathable, lightweight, and stretchy. Plus, they look way more stylish. Check out their Ota high top. It's got a thicker snow gripping also, a warm liner to keep your feet cozy and comfy. Whether you are chasing your New Year's goals or running away from your responsibilities in the rim, Vessy's got you We have been working with Vasily for years and so many of our team members actually wear Vasily every single day, including myself. Check out vasily.com slash LTT and get an automatic 15% off your first order at checkout. Thanks for watching, guys. If you liked this video, check out our 5090 review or check out lttlabs.com for even more info on these new cards. The site is always getting updated.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def process_transcript(transcript_text, target_lang=\"en\"):\n",
        "    print(\"\\nSummarizing transcript in English...\")\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    try:\n",
        "        summary_english = summarizer(transcript_text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Error during summarization: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    if target_lang != \"en\":\n",
        "        print(f\"\\nTranslating summary to {target_lang}...\")\n",
        "        model_name = f\"Helsinki-NLP/opus-mt-en-{target_lang}\"\n",
        "        try:\n",
        "            translator = pipeline(\"translation\", model=model_name)\n",
        "            translation_result = translator(summary_english)\n",
        "            summary_final = translation_result[0]['translation_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error during translation: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        summary_final = summary_english\n",
        "\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(summary_final)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T18:57:42.722373Z",
          "iopub.execute_input": "2025-02-05T18:57:42.722693Z",
          "iopub.status.idle": "2025-02-05T18:57:42.728470Z",
          "shell.execute_reply.started": "2025-02-05T18:57:42.722666Z",
          "shell.execute_reply": "2025-02-05T18:57:42.727470Z"
        },
        "id": "IlmotpucjNvw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "process_transcript(transcript, target_lang=\"es\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T19:02:52.433992Z",
          "iopub.execute_input": "2025-02-05T19:02:52.434321Z",
          "iopub.status.idle": "2025-02-05T19:03:04.423897Z",
          "shell.execute_reply.started": "2025-02-05T19:02:52.434294Z",
          "shell.execute_reply": "2025-02-05T19:03:04.422733Z"
        },
        "id": "93FjGNfKjNvz"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}